{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/canozensoy/recurrent-neural-network?scriptVersionId=228456355\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"15cbfe2f","metadata":{"id":"267JaSf8LXh1","papermill":{"duration":0.005159,"end_time":"2025-03-19T10:25:39.580226","exception":false,"start_time":"2025-03-19T10:25:39.575067","status":"completed"},"tags":[]},"source":["This code preprocesses text data by converting sentences into numerical sequences using a Tokenizer, padding those sequences to a fixed length, and then transforming the integer sequences into dense vector representations using an Embedding layer. This process creates word embeddings, which are numerical representations of words that capture semantic relationships, making them suitable for input to neural networks for various natural language processing tasks."]},{"cell_type":"markdown","id":"33d5a8e8","metadata":{"papermill":{"duration":0.003951,"end_time":"2025-03-19T10:25:39.588997","exception":false,"start_time":"2025-03-19T10:25:39.585046","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Tokenizer**\n","\n","The code effectively performs the following:\n","\n","It takes a list of sentences.\n","\n","It creates a vocabulary from the unique words in those sentences.\n","\n","It assigns a unique numerical index to each word in the vocabulary.\n","\n","It converts the sentences into sequences of integers, where each integer corresponds to the index of a word.\n","\n","It displays the word index dictionary, showing the word to index mapping.\n","\n","This is a fundamental step in many natural language processing (NLP) tasks, as machine learning models typically work with numerical data."]},{"cell_type":"code","execution_count":1,"id":"c5b9915e","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:39.599067Z","iopub.status.busy":"2025-03-19T10:25:39.598673Z","iopub.status.idle":"2025-03-19T10:25:57.119415Z","shell.execute_reply":"2025-03-19T10:25:57.117879Z"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1736765510597,"user":{"displayName":"Can Özensoy","userId":"04013485789002617233"},"user_tz":-180},"id":"srXl2v2yLOey","outputId":"2249e7f4-f5e3-4e6b-efbe-00498d4bf43f","papermill":{"duration":17.528258,"end_time":"2025-03-19T10:25:57.121556","exception":false,"start_time":"2025-03-19T10:25:39.593298","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Word index dictionary:  {'is': 1, 'example': 2, 'sentence': 3, 'this': 4, 'an': 5, 'there': 6, 'another': 7, 'a': 8, 'different': 9, 'text': 10}\n"]}],"source":["from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","metin = [\n","    'This is an example sentence.',\n","    'There is another sentence.',\n","    'A different example text.'\n","]\n","# Creating a tokenizer and adapting it to text data.\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(metin)\n","\n","# Assign index\n","sequences = tokenizer.texts_to_sequences(metin)\n","\n","word_index = tokenizer.word_index\n","print(\"Word index dictionary: \", word_index)"]},{"cell_type":"markdown","id":"d6de7218","metadata":{"papermill":{"duration":0.004282,"end_time":"2025-03-19T10:25:57.130631","exception":false,"start_time":"2025-03-19T10:25:57.126349","status":"completed"},"tags":[]},"source":["This loop iterates through each element in the sequences list. Each element (seq) represents a sequence of integers, where each integer corresponds to a word index."]},{"cell_type":"code","execution_count":2,"id":"be308e0d","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.141185Z","iopub.status.busy":"2025-03-19T10:25:57.140563Z","iopub.status.idle":"2025-03-19T10:25:57.146413Z","shell.execute_reply":"2025-03-19T10:25:57.145022Z"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1736765562623,"user":{"displayName":"Can Özensoy","userId":"04013485789002617233"},"user_tz":-180},"id":"_s6mdKMOSyDA","outputId":"f493a01d-3f84-424a-fa06-dc67a1cd45e8","papermill":{"duration":0.013118,"end_time":"2025-03-19T10:25:57.148223","exception":false,"start_time":"2025-03-19T10:25:57.135105","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 1, 5, 2, 3]\n","[6, 1, 7, 3]\n","[8, 9, 2, 10]\n"]}],"source":["for seq in sequences:\n","  print(seq)"]},{"cell_type":"markdown","id":"8f7da14c","metadata":{"papermill":{"duration":0.003984,"end_time":"2025-03-19T10:25:57.156639","exception":false,"start_time":"2025-03-19T10:25:57.152655","status":"completed"},"tags":[]},"source":["This code snippet demonstrates how to pad sequences to a fixed length using Keras's pad_sequences function.\n","\n","<font color = \"DeepSkyBlue\">**max_sequence_length = 5:** <font color = \"White\">This line sets the maximum length for all sequences to 5. If a sequence is shorter than 5, it will be padded; if it's longer, it will be truncated.\n","\n","<font color = \"DeepSkyBlue\">**pad_sequences(sequences, maxlen=max_sequence_length):**<font color = \"White\">\n","\n","- This function takes the sequences (the list of integer sequences generated by the Tokenizer) and pads them to the specified max_sequence_length. \n","\n","- By default, padding is done at the beginning of the sequences (pre-padding) with zeros.\n","\n","- The result is a new list of padded sequences, which is assigned to the padded_sequences variable.\n","\n","<font color = \"DeepSkyBlue\">**for seq in padded_sequences:** <font color = \"White\">This loop iterates through each padded sequence in the padded_sequences list.\n","\n","<font color = \"DeepSkyBlue\">**print(seq):** <font color = \"White\">This line prints each padded sequence to the console."]},{"cell_type":"code","execution_count":3,"id":"b7aa4160","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.166194Z","iopub.status.busy":"2025-03-19T10:25:57.165869Z","iopub.status.idle":"2025-03-19T10:25:57.172901Z","shell.execute_reply":"2025-03-19T10:25:57.17158Z"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1736766471322,"user":{"displayName":"Can Özensoy","userId":"04013485789002617233"},"user_tz":-180},"id":"D7-q9COXToF6","outputId":"8a4ac977-6829-4586-aa6e-087018566975","papermill":{"duration":0.014033,"end_time":"2025-03-19T10:25:57.174816","exception":false,"start_time":"2025-03-19T10:25:57.160783","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[4 1 5 2 3]\n","[0 6 1 7 3]\n","[ 0  8  9  2 10]\n"]}],"source":["# Fixed length\n","max_sequence_length = 5\n","padded_sequences = pad_sequences(sequences, maxlen = max_sequence_length)\n","\n","for seq in padded_sequences:\n","  print(seq)"]},{"cell_type":"markdown","id":"60c7ad22","metadata":{"id":"dUogBUtlbqNF","papermill":{"duration":0.004051,"end_time":"2025-03-19T10:25:57.183461","exception":false,"start_time":"2025-03-19T10:25:57.17941","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Embedding**"]},{"cell_type":"markdown","id":"53cb7bcc","metadata":{"papermill":{"duration":0.003844,"end_time":"2025-03-19T10:25:57.191475","exception":false,"start_time":"2025-03-19T10:25:57.187631","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Calculate the vocabulary size**\n","\n","The vocabulary size is the number of unique words in the text data, plus one for padding/unknown tokens."]},{"cell_type":"code","execution_count":4,"id":"32446c7e","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.201165Z","iopub.status.busy":"2025-03-19T10:25:57.200834Z","iopub.status.idle":"2025-03-19T10:25:57.207286Z","shell.execute_reply":"2025-03-19T10:25:57.206044Z"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1736768281791,"user":{"displayName":"Can Özensoy","userId":"04013485789002617233"},"user_tz":-180},"id":"8l0cBxEbbruE","outputId":"aa01e485-4119-4244-f8b6-11869400af03","papermill":{"duration":0.013701,"end_time":"2025-03-19T10:25:57.209308","exception":false,"start_time":"2025-03-19T10:25:57.195607","status":"completed"},"tags":[]},"outputs":[],"source":["from keras.layers import Embedding\n","import numpy as np\n","\n","size = len(tokenizer.word_index) + 1\n"]},{"cell_type":"markdown","id":"11c194cd","metadata":{"papermill":{"duration":0.003776,"end_time":"2025-03-19T10:25:57.217452","exception":false,"start_time":"2025-03-19T10:25:57.213676","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Define the embedding dimension**\n","\n","The embedding dimension is the size of the dense vector representation for each word.\n","\n","A higher dimension can capture more complex semantic relationships, but also increases computational cost."]},{"cell_type":"code","execution_count":5,"id":"8f6288c4","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.228124Z","iopub.status.busy":"2025-03-19T10:25:57.227679Z","iopub.status.idle":"2025-03-19T10:25:57.232308Z","shell.execute_reply":"2025-03-19T10:25:57.231116Z"},"papermill":{"duration":0.01193,"end_time":"2025-03-19T10:25:57.234204","exception":false,"start_time":"2025-03-19T10:25:57.222274","status":"completed"},"tags":[]},"outputs":[],"source":["dimension = 100\n"]},{"cell_type":"markdown","id":"39edad60","metadata":{"papermill":{"duration":0.004033,"end_time":"2025-03-19T10:25:57.24249","exception":false,"start_time":"2025-03-19T10:25:57.238457","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Create the Embedding layer**\n","\n","The Embedding layer maps integer indices to dense vectors.\n","\n","<font color = \"DeepSkyBlue\">**input_dim:** <font color = \"White\">The size of the vocabulary (number of unique tokens).\n","\n","<font color = \"DeepSkyBlue\">**output_dim:** <font color = \"White\">The dimension of the dense embedding.\n","\n","<font color = \"DeepSkyBlue\">**input_length:** <font color = \"White\">The length of the input sequences (padded sequences)."]},{"cell_type":"code","execution_count":6,"id":"bfcf4757","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.252593Z","iopub.status.busy":"2025-03-19T10:25:57.252109Z","iopub.status.idle":"2025-03-19T10:25:57.260687Z","shell.execute_reply":"2025-03-19T10:25:57.259101Z"},"papermill":{"duration":0.015914,"end_time":"2025-03-19T10:25:57.262534","exception":false,"start_time":"2025-03-19T10:25:57.24662","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]}],"source":["embedding_layer = Embedding(input_dim = size, output_dim = dimension, input_length = max_sequence_length)\n"]},{"cell_type":"markdown","id":"445d90ac","metadata":{"papermill":{"duration":0.00416,"end_time":"2025-03-19T10:25:57.271357","exception":false,"start_time":"2025-03-19T10:25:57.267197","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Apply the Embedding layer to the padded sequences**\n","\n","Converts the padded sequences from integer indices to dense vector representations.\n","\n","<font color = \"DeepSkyBlue\">**padded_sequences:** <font color = \"White\">The sequences of integer indices, padded to a fixed length.\n","\n","<font color = \"DeepSkyBlue\">**np.array(...):** <font color = \"White\">Converts the list of sequences to a NumPy array, which is required by the Embedding layer."]},{"cell_type":"code","execution_count":7,"id":"79c09fab","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.28155Z","iopub.status.busy":"2025-03-19T10:25:57.281125Z","iopub.status.idle":"2025-03-19T10:25:57.38207Z","shell.execute_reply":"2025-03-19T10:25:57.380705Z"},"papermill":{"duration":0.108815,"end_time":"2025-03-19T10:25:57.384442","exception":false,"start_time":"2025-03-19T10:25:57.275627","status":"completed"},"tags":[]},"outputs":[],"source":["embedded_sequences = embedding_layer(np.array(padded_sequences))\n"]},{"cell_type":"markdown","id":"972a48e5","metadata":{"papermill":{"duration":0.004969,"end_time":"2025-03-19T10:25:57.407071","exception":false,"start_time":"2025-03-19T10:25:57.402102","status":"completed"},"tags":[]},"source":["<font color = \"DeepSkyBlue\">**Print the embedding result**\n","\n","Displays the embedded sequences, which are now represented as dense vectors.\n","\n","<font color = \"DeepSkyBlue\">**The output is a 3D tensor:** <font color = \"White\">(number of sequences, sequence length, embedding dimension)."]},{"cell_type":"code","execution_count":8,"id":"ff447df1","metadata":{"execution":{"iopub.execute_input":"2025-03-19T10:25:57.41839Z","iopub.status.busy":"2025-03-19T10:25:57.417974Z","iopub.status.idle":"2025-03-19T10:25:57.425819Z","shell.execute_reply":"2025-03-19T10:25:57.424093Z"},"papermill":{"duration":0.016072,"end_time":"2025-03-19T10:25:57.427879","exception":false,"start_time":"2025-03-19T10:25:57.411807","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Embedding result: \n","tf.Tensor(\n","[[[-0.00440811  0.0367333  -0.04424722 ...  0.02052054 -0.0331463\n","   -0.00938786]\n","  [-0.03940564 -0.03481667 -0.0252624  ...  0.04283511 -0.0337648\n","   -0.01106616]\n","  [-0.02147403 -0.03271443  0.01033436 ... -0.01358676  0.00922159\n","    0.00468035]\n","  [ 0.03005802  0.00400939 -0.00136522 ...  0.03290996 -0.01794069\n","   -0.01546265]\n","  [-0.01146143  0.03761804 -0.02635258 ...  0.02882895  0.0406894\n","    0.03326086]]\n","\n"," [[-0.03371916  0.00113312 -0.00088071 ... -0.02993964  0.01448205\n","   -0.02482233]\n","  [-0.02695836  0.01384885  0.02952087 ...  0.01972664  0.03502656\n","    0.01162883]\n","  [-0.03940564 -0.03481667 -0.0252624  ...  0.04283511 -0.0337648\n","   -0.01106616]\n","  [-0.02588189 -0.02417557 -0.00342847 ... -0.01197594  0.04132814\n","   -0.02149092]\n","  [-0.01146143  0.03761804 -0.02635258 ...  0.02882895  0.0406894\n","    0.03326086]]\n","\n"," [[-0.03371916  0.00113312 -0.00088071 ... -0.02993964  0.01448205\n","   -0.02482233]\n","  [ 0.01072764  0.04473083 -0.01721001 ... -0.0154246  -0.00420188\n","    0.00372501]\n","  [-0.03479506 -0.01506603 -0.04133788 ...  0.00905998 -0.04223117\n","    0.0315686 ]\n","  [ 0.03005802  0.00400939 -0.00136522 ...  0.03290996 -0.01794069\n","   -0.01546265]\n","  [-0.01023892 -0.02791717  0.02510491 ...  0.04893093 -0.04763974\n","    0.01612414]]], shape=(3, 5, 100), dtype=float32)\n"]}],"source":["print(\"Embedding result: \")\n","print(embedded_sequences)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO/H1FWL0vsQOeEOFhxmAFw","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":23.837131,"end_time":"2025-03-19T10:26:00.223958","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-19T10:25:36.386827","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}